"""
å°é“ Agent æ ¸å¿ƒ

å®ç° Agent æ‰§è¡Œå¾ªç¯ï¼š
1. æ¥æ”¶ç”¨æˆ·è¾“å…¥
2. è°ƒç”¨ LLM ç”Ÿæˆå“åº”
3. æ‰§è¡Œå·¥å…·è°ƒç”¨
4. å¾ªç¯ç›´åˆ°ä»»åŠ¡å®Œæˆ
"""

from __future__ import annotations

import asyncio
import sys
import time
from typing import Any, Optional, List, Dict, Callable

try:
    import tiktoken
    HAS_TIKTOKEN = True
except ImportError:
    HAS_TIKTOKEN = False

from .schema import Message, LLMResponse
from .llm import LLMClient
from .tools import Tool


class Agent:
    """å°é“ Agent"""

    def __init__(
        self,
        llm_client: LLMClient,
        system_prompt: str,
        tools: list[Tool],
        max_steps: int = 50,
        token_limit: int = 100000,
        workspace_dir: str = ".",
        stream: bool = True,
        enable_thinking: bool = True,
        quiet: bool = False,  # é™é»˜æ¨¡å¼ï¼Œä¸æ‰“å°å·¥å…·æ‰§è¡Œä¿¡æ¯
    ):
        self.llm = llm_client
        self.tools: dict[str, Tool] = {t.name: t for t in tools}
        self.max_steps = max_steps
        self.token_limit = token_limit
        self.workspace_dir = workspace_dir
        self.stream = stream
        self.enable_thinking = enable_thinking
        self.quiet = quiet
        self.parallel_tools = True  # å¹¶è¡Œæ‰§è¡Œå·¥å…·

        # æ¶ˆæ¯å†å²
        self.messages: list[Message] = [
            Message(role="system", content=system_prompt)
        ]

        # å–æ¶ˆäº‹ä»¶
        self.cancel_event: Optional[asyncio.Event] = None

        # Token ç»Ÿè®¡
        self.api_total_tokens = 0

        # tiktoken ç¼–ç å™¨
        self._encoding = None
        if HAS_TIKTOKEN:
            try:
                self._encoding = tiktoken.get_encoding("cl100k_base")
            except Exception:
                pass

        # è¾“å‡ºå›è°ƒ
        self.on_thinking: Optional[Callable[[str], None]] = None
        self.on_content: Optional[Callable[[str], None]] = None

    def _check_cancelled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¢«å–æ¶ˆ"""
        if self.cancel_event is not None and self.cancel_event.is_set():
            return True
        return False

    def _cleanup_incomplete_messages(self):
        """æ¸…ç†æœªå®Œæˆçš„æ¶ˆæ¯ï¼ˆå–æ¶ˆæ—¶è°ƒç”¨ï¼‰"""
        # æ‰¾åˆ°æœ€åä¸€ä¸ª assistant æ¶ˆæ¯
        last_assistant_idx = -1
        for i in range(len(self.messages) - 1, -1, -1):
            if self.messages[i].role == "assistant":
                last_assistant_idx = i
                break

        # å¦‚æœæœ‰æœªå®Œæˆçš„ tool è°ƒç”¨ï¼Œåˆ é™¤ assistant åŠå…¶åçš„æ¶ˆæ¯
        if last_assistant_idx >= 0:
            assistant_msg = self.messages[last_assistant_idx]
            if assistant_msg.tool_calls:
                # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ tool è°ƒç”¨éƒ½æœ‰ç»“æœ
                tool_call_ids = {tc.id for tc in assistant_msg.tool_calls}
                result_ids = set()
                for msg in self.messages[last_assistant_idx + 1:]:
                    if msg.role == "tool" and msg.tool_call_id:
                        result_ids.add(msg.tool_call_id)

                if tool_call_ids != result_ids:
                    # æœ‰æœªå®Œæˆçš„è°ƒç”¨ï¼Œåˆ é™¤
                    self.messages = self.messages[:last_assistant_idx]

    def _estimate_tokens(self) -> int:
        """ä¼°ç®—å½“å‰æ¶ˆæ¯çš„ token æ•°"""
        if self._encoding is None:
            # æ²¡æœ‰ tiktokenï¼ŒæŒ‰å­—ç¬¦ä¼°ç®—
            total_chars = sum(
                len(str(msg.content)) + len(str(msg.thinking or ""))
                for msg in self.messages
            )
            return total_chars // 4

        total = 0
        for msg in self.messages:
            if isinstance(msg.content, str):
                total += len(self._encoding.encode(msg.content))
            if msg.thinking:
                total += len(self._encoding.encode(msg.thinking))
        return total

    async def _summarize_messages(self):
        """å½“ token è¶…é™æ—¶æ‘˜è¦å†å²æ¶ˆæ¯"""
        estimated = self._estimate_tokens()
        if estimated <= self.token_limit and self.api_total_tokens <= self.token_limit:
            return

        print(f"âš ï¸ Token è¶…é™ ({estimated}/{self.token_limit})ï¼Œæ­£åœ¨æ‘˜è¦...")

        # ä¿ç•™ system æ¶ˆæ¯
        system_msg = self.messages[0] if self.messages[0].role == "system" else None
        new_messages = [system_msg] if system_msg else []

        # æ”¶é›†éœ€è¦æ‘˜è¦çš„å†…å®¹
        content_to_summarize = []
        for msg in self.messages[1:]:
            if msg.role == "user":
                # ä¿ç•™ç”¨æˆ·æ¶ˆæ¯
                new_messages.append(msg)
            else:
                # æ”¶é›† assistant å’Œ tool æ¶ˆæ¯
                if msg.content:
                    content_to_summarize.append(f"[{msg.role}]: {msg.content[:500]}")

        if content_to_summarize:
            # ç”Ÿæˆæ‘˜è¦
            summary_prompt = f"è¯·ç”¨ä¸­æ–‡ç®€æ´æ‘˜è¦ä»¥ä¸‹å¯¹è¯å†…å®¹ï¼ˆä¿ç•™å…³é”®ä¿¡æ¯ï¼‰:\n\n" + "\n".join(content_to_summarize[-20:])
            summary_response = await self.llm.generate([
                Message(role="user", content=summary_prompt)
            ])
            summary = summary_response.content

            # æ·»åŠ æ‘˜è¦æ¶ˆæ¯
            new_messages.append(Message(
                role="assistant",
                content=f"[å†å²æ‘˜è¦]\n{summary}"
            ))

        self.messages = new_messages
        print(f"âœ… æ‘˜è¦å®Œæˆï¼Œæ¶ˆæ¯æ•°: {len(self.messages)}")

    async def run(self, user_input: Optional[str] = None) -> str:
        """è¿è¡Œ Agent"""
        # æ·»åŠ ç”¨æˆ·è¾“å…¥
        if user_input:
            self.messages.append(Message(role="user", content=user_input))

        for step in range(self.max_steps):
            # æ£€æŸ¥å–æ¶ˆ
            if self._check_cancelled():
                self._cleanup_incomplete_messages()
                return "âš ï¸ ä»»åŠ¡å·²å–æ¶ˆ"

            # Token ç®¡ç†
            await self._summarize_messages()

            # è·å–å·¥å…· schema
            tool_schemas = [tool.to_schema() for tool in self.tools.values()]

            # è°ƒç”¨ LLM
            try:
                if self.stream:
                    response = await self._stream_generate(tool_schemas)
                else:
                    response = await self.llm.generate(
                        messages=self.messages,
                        tools=tool_schemas if tool_schemas else None,
                    )
            except Exception as e:
                return f"âŒ LLM è°ƒç”¨å¤±è´¥: {e}"

            # æ›´æ–° token ç»Ÿè®¡
            if response.usage:
                self.api_total_tokens = response.usage.total_tokens

            # æ·»åŠ  assistant æ¶ˆæ¯
            self.messages.append(Message(
                role="assistant",
                content=response.content,
                thinking=response.thinking,
                tool_calls=response.tool_calls,
            ))

            # å¦‚æœæ²¡æœ‰å·¥å…·è°ƒç”¨ï¼Œä»»åŠ¡å®Œæˆ
            if not response.tool_calls:
                return response.content

            # æ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆå¹¶è¡Œæ‰§è¡Œï¼‰
            tool_results = await self._execute_tools_parallel(response.tool_calls)

            # æ·»åŠ å·¥å…·ç»“æœåˆ°æ¶ˆæ¯å†å²
            for tool_call_id, function_name, result_content in tool_results:
                self.messages.append(Message(
                    role="tool",
                    content=result_content,
                    tool_call_id=tool_call_id,
                    name=function_name,
                ))

        return "âš ï¸ è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶"

    async def _execute_tools_parallel(
        self, tool_calls: list
    ) -> list[tuple[str, str, str]]:
        """å¹¶è¡Œæ‰§è¡Œå¤šä¸ªå·¥å…·è°ƒç”¨

        Returns:
            list of (tool_call_id, function_name, result_content)
        """
        if self._check_cancelled():
            self._cleanup_incomplete_messages()
            return []

        async def execute_single_tool(tool_call) -> tuple[str, str, str]:
            """æ‰§è¡Œå•ä¸ªå·¥å…·"""
            tool_call_id = tool_call.id
            function_name = tool_call.function.name
            arguments = tool_call.function.arguments

            # æ ¼å¼åŒ–å‚æ•°æ˜¾ç¤º
            if not self.quiet:
                args_display = ", ".join(
                    f"{k}={repr(v)[:50]}" for k, v in arguments.items()
                )
                print(f"\nğŸ”§ {function_name}({args_display})")

            tool = self.tools.get(function_name)
            if not tool:
                result_content = f"é”™è¯¯: æœªçŸ¥å·¥å…· '{function_name}'"
                if not self.quiet:
                    print(f"   âŒ {result_content}")
                return (tool_call_id, function_name, result_content)

            try:
                start_time = time.time()
                result = await tool.execute(**arguments)
                elapsed = time.time() - start_time

                if result.success:
                    result_content = result.content
                    # æ˜¾ç¤ºç»“æœé¢„è§ˆ
                    if not self.quiet:
                        preview = result_content[:100].replace("\n", " ")
                        if len(result_content) > 100:
                            preview += "..."
                        print(f"   âœ… ({elapsed:.1f}s) {preview}")
                else:
                    result_content = f"é”™è¯¯: {result.error}"
                    if not self.quiet:
                        print(f"   âŒ ({elapsed:.1f}s) {result.error}")
            except Exception as e:
                result_content = f"æ‰§è¡Œå¼‚å¸¸: {e}"
                if not self.quiet:
                    print(f"   âŒ {result_content}")

            return (tool_call_id, function_name, result_content)

        # å¹¶è¡Œæˆ–ä¸²è¡Œæ‰§è¡Œå·¥å…·è°ƒç”¨
        if self.parallel_tools and len(tool_calls) > 1:
            if not self.quiet:
                print(f"\nâš¡ å¹¶è¡Œæ‰§è¡Œ {len(tool_calls)} ä¸ªå·¥å…·...")
            start_time = time.time()
            tasks = [execute_single_tool(tc) for tc in tool_calls]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            elapsed = time.time() - start_time
            if not self.quiet:
                print(f"   â±ï¸ å®Œæˆï¼Œæ€»è€—æ—¶ {elapsed:.2f}s")
        else:
            # ä¸²è¡Œæ‰§è¡Œ
            results = []
            for tc in tool_calls:
                result = await execute_single_tool(tc)
                results.append(result)

        # å¤„ç†ç»“æœ
        final_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                tc = tool_calls[i]
                final_results.append((
                    tc.id,
                    tc.function.name,
                    f"æ‰§è¡Œå¼‚å¸¸: {result}"
                ))
            else:
                final_results.append(result)

        return final_results

    async def _stream_generate(self, tool_schemas: list) -> LLMResponse:
        """æµå¼ç”Ÿæˆå“åº”"""
        thinking_started = False
        content_started = False

        def on_thinking(text: str):
            nonlocal thinking_started
            if self.quiet:
                return
            if not thinking_started:
                print("\nğŸ’­ æ€è€ƒä¸­...", flush=True)
                thinking_started = True
            # å¯é€‰ï¼šæ˜¾ç¤ºæ€è€ƒè¿‡ç¨‹
            # print(text, end="", flush=True)

        def on_content(text: str):
            nonlocal content_started
            if self.quiet:
                return
            if not content_started:
                print("\nğŸ¤– å°é“:", flush=True)
                content_started = True
            print(text, end="", flush=True)

        response = await self.llm.generate_stream(
            messages=self.messages,
            tools=tool_schemas if tool_schemas else None,
            on_thinking=on_thinking,
            on_content=on_content,
            enable_thinking=self.enable_thinking,
        )

        if content_started and not self.quiet:
            print()  # æ¢è¡Œ

        return response

    def reset(self):
        """é‡ç½® Agent çŠ¶æ€"""
        system_msg = self.messages[0] if self.messages and self.messages[0].role == "system" else None
        self.messages = [system_msg] if system_msg else []
        self.api_total_tokens = 0
